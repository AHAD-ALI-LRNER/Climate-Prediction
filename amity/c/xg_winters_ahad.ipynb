{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\2757474805.py:70: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m df_city \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m city]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     28\u001b[0m df_city\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 29\u001b[0m df_city \u001b[38;5;241m=\u001b[39m \u001b[43mdf_city\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masfreq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Fill missing values using linear interpolation\u001b[39;00m\n\u001b[0;32m     32\u001b[0m df_city[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex Value\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_city[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex Value\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39minterpolate(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Anacondaa\\Lib\\site-packages\\pandas\\core\\generic.py:9231\u001b[0m, in \u001b[0;36mNDFrame.asfreq\u001b[1;34m(self, freq, method, how, normalize, fill_value)\u001b[0m\n\u001b[0;32m   9124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9125\u001b[0m \u001b[38;5;124;03mConvert time series to specified frequency.\u001b[39;00m\n\u001b[0;32m   9126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9227\u001b[0m \u001b[38;5;124;03m2000-01-01 00:03:00    3.0\u001b[39;00m\n\u001b[0;32m   9228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9229\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asfreq\n\u001b[1;32m-> 9231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masfreq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anacondaa\\Lib\\site-packages\\pandas\\core\\resample.py:2837\u001b[0m, in \u001b[0;36masfreq\u001b[1;34m(obj, freq, method, how, normalize, fill_value)\u001b[0m\n\u001b[0;32m   2835\u001b[0m dti \u001b[38;5;241m=\u001b[39m date_range(obj\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmin(), obj\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmax(), freq\u001b[38;5;241m=\u001b[39mfreq, unit\u001b[38;5;241m=\u001b[39munit)\n\u001b[0;32m   2836\u001b[0m dti\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m-> 2837\u001b[0m new_obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m   2839\u001b[0m     new_obj\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m new_obj\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnormalize()\n",
      "File \u001b[1;32md:\\Anacondaa\\Lib\\site-packages\\pandas\\core\\frame.py:5378\u001b[0m, in \u001b[0;36mDataFrame.reindex\u001b[1;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5359\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m   5360\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,\n\u001b[0;32m   5361\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5376\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5377\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m-> 5378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5382\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anacondaa\\Lib\\site-packages\\pandas\\core\\generic.py:5610\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[0;32m   5609\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 5610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5611\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m   5612\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anacondaa\\Lib\\site-packages\\pandas\\core\\generic.py:5633\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   5630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   5632\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 5633\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[0;32m   5635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5637\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n\u001b[0;32m   5638\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   5639\u001b[0m     {axis: [new_index, indexer]},\n\u001b[0;32m   5640\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m   5641\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   5642\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   5643\u001b[0m )\n",
      "File \u001b[1;32md:\\Anacondaa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4429\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot handle a non-unique multi-index!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m   4428\u001b[0m     \u001b[38;5;66;03m# GH#42568\u001b[39;00m\n\u001b[1;32m-> 4429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4431\u001b[0m     indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load dataset\n",
    "df=pd.read_csv(r'C:\\Users\\ASUS\\Desktop\\Climate1\\AQI.csv')  # Update with your file path\n",
    "\n",
    "# Convert date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Count unique days per city\n",
    "city_data_counts = df.groupby('City')['date'].nunique()\n",
    "\n",
    "# Categorize cities\n",
    "cities_with_2_years_data = city_data_counts[city_data_counts > 730].index.tolist()\n",
    "cities_with_less_than_2_years_data = city_data_counts[city_data_counts <= 730].index.tolist()\n",
    "\n",
    "# Store prediction results\n",
    "prediction_results = []\n",
    "\n",
    "for city in df['City'].unique():\n",
    "    df_city = df[df['City'] == city].copy()\n",
    "    df_city.set_index('date', inplace=True)\n",
    "    df_city = df_city.asfreq('D')\n",
    "    \n",
    "    # Fill missing values using linear interpolation\n",
    "    df_city['Index Value'] = df_city['Index Value'].interpolate(method='linear')\n",
    "    \n",
    "    # Use Iterative Imputer for consecutive missing values\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    df_city[['Index Value']] = imputer.fit_transform(df_city[['Index Value']])\n",
    "    \n",
    "    # Define features and target\n",
    "    df_city['day_of_year'] = df_city.index.dayofyear\n",
    "    df_city['year'] = df_city.index.year\n",
    "    X = df_city[['day_of_year', 'year']]\n",
    "    y = df_city['Index Value']\n",
    "    \n",
    "    # Split data for training/testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    if city in cities_with_2_years_data:\n",
    "        # Apply Holt-Winters\n",
    "        seasonal_periods = min(365, len(df_city) // 2)\n",
    "        model_hw = ExponentialSmoothing(y_train, trend='add', seasonal='add', seasonal_periods=seasonal_periods).fit()\n",
    "        hw_pred = model_hw.forecast(len(X_test))\n",
    "        X_train['hw_prediction'] = model_hw.fittedvalues\n",
    "        X_test['hw_prediction'] = hw_pred\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    model_xgb = XGBRegressor(objective='reg:squarederror', n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    xgb_pred = model_xgb.predict(X_test)\n",
    "    \n",
    "    # Final prediction\n",
    "    final_prediction = xgb_pred if city in cities_with_less_than_2_years_data else (0.5 * xgb_pred + 0.5 * hw_pred)\n",
    "    \n",
    "    # Compute confidence\n",
    "    mae = mean_absolute_error(y_test, final_prediction)\n",
    "    confidence = max(0, 100 - (mae / np.mean(y_test)) * 100)\n",
    "    \n",
    "    # Store results\n",
    "    prediction_results.append({\n",
    "        \"City\": city,\n",
    "        \"Predicted AQI\": round(final_prediction[-1], 2),\n",
    "        \"Prediction Confidence\": round(confidence, 2)\n",
    "    })\n",
    "\n",
    "# Output predictions\n",
    "for result in prediction_results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6716\\822779639.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"Predicted AQI\": round(final_prediction[-1], 2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'City': 'Chennai', 'Predicted AQI': -48.31, 'Prediction Confidence': 0}\n",
      "{'City': 'Varanasi', 'Predicted AQI': 666.87, 'Prediction Confidence': 0}\n",
      "{'City': 'Hyderabad', 'Predicted AQI': 153.58, 'Prediction Confidence': 68.88}\n",
      "{'City': 'Agra', 'Predicted AQI': 712.28, 'Prediction Confidence': 0}\n",
      "{'City': 'Pune', 'Predicted AQI': 449.72, 'Prediction Confidence': 0}\n",
      "{'City': 'Mumbai', 'Predicted AQI': 169.42, 'Prediction Confidence': 69.26}\n",
      "{'City': 'Patna', 'Predicted AQI': -250.31, 'Prediction Confidence': 0}\n",
      "{'City': 'Jodhpur', 'Predicted AQI': 336.38, 'Prediction Confidence': 8.33}\n",
      "{'City': 'Gaya', 'Predicted AQI': -175.54, 'Prediction Confidence': 0}\n",
      "{'City': 'Nagpur', 'Predicted AQI': 299.45, 'Prediction Confidence': 20.55}\n",
      "{'City': 'Visakhapatnam', 'Predicted AQI': 147.31, 'Prediction Confidence': 64.99}\n",
      "{'City': 'Amritsar', 'Predicted AQI': 273.4, 'Prediction Confidence': 46.54}\n",
      "{'City': 'Thiruvananthapuram', 'Predicted AQI': 23.35, 'Prediction Confidence': 51.7}\n",
      "{'City': 'Ajmer', 'Predicted AQI': 112.54, 'Prediction Confidence': 71.7}\n",
      "{'City': 'Greater Noida', 'Predicted AQI': 16.18, 'Prediction Confidence': 17.81}\n",
      "{'City': 'Guwahati', 'Predicted AQI': -66.9, 'Prediction Confidence': 0}\n",
      "{'City': 'Chandigarh', 'Predicted AQI': 1610.06, 'Prediction Confidence': 0}\n",
      "{'City': 'Indore', 'Predicted AQI': -233.76, 'Prediction Confidence': 0}\n",
      "{'City': 'Aizawl', 'Predicted AQI': -232.27, 'Prediction Confidence': 0}\n",
      "{'City': 'Agartala', 'Predicted AQI': -296.99, 'Prediction Confidence': 0}\n",
      "{'City': 'Mangalore', 'Predicted AQI': 29.91, 'Prediction Confidence': 65.25}\n",
      "{'City': 'Puducherry', 'Predicted AQI': 74.67, 'Prediction Confidence': 61.74}\n",
      "{'City': 'Srinagar', 'Predicted AQI': 301.5, 'Prediction Confidence': 0}\n",
      "{'City': 'Araria', 'Predicted AQI': 318.28, 'Prediction Confidence': 71.42}\n",
      "{'City': 'Gangtok', 'Predicted AQI': 85.54, 'Prediction Confidence': 86.42}\n",
      "{'City': 'Dehradun', 'Predicted AQI': 192.65, 'Prediction Confidence': 71.1}\n",
      "{'City': 'Aurangabad (Bihar)', 'Predicted AQI': 308.76, 'Prediction Confidence': 42.03}\n",
      "{'City': 'Bikaner', 'Predicted AQI': 271.02, 'Prediction Confidence': 77.07}\n",
      "{'City': 'Jaisalmer', 'Predicted AQI': 189.72, 'Prediction Confidence': 71.78}\n",
      "{'City': 'Bhubaneswar', 'Predicted AQI': 252.37, 'Prediction Confidence': 90.64}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load dataset\n",
    "  # Update with your file path\n",
    "\n",
    "# Convert date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Remove duplicate rows based on 'City' and 'date'\n",
    "df = df.drop_duplicates(subset=['City', 'date'])\n",
    "\n",
    "# Count unique days per city\n",
    "city_data_counts = df.groupby('City')['date'].nunique()\n",
    "\n",
    "# Categorize cities\n",
    "cities_with_2_years_data = city_data_counts[city_data_counts > 730].index.tolist()\n",
    "cities_with_less_than_2_years_data = city_data_counts[city_data_counts <= 730].index.tolist()\n",
    "\n",
    "# Store prediction results\n",
    "prediction_results = []\n",
    "\n",
    "for city in df['City'].unique():\n",
    "    df_city = df[df['City'] == city].copy()\n",
    "    df_city.set_index('date', inplace=True)\n",
    "    df_city = df_city[~df_city.index.duplicated(keep='first')]\n",
    "    df_city = df_city.asfreq('D')\n",
    "    \n",
    "    # Fill missing values using linear interpolation\n",
    "    df_city['Index Value'] = df_city['Index Value'].interpolate(method='linear')\n",
    "    \n",
    "    # Use Iterative Imputer for consecutive missing values\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    df_city[['Index Value']] = imputer.fit_transform(df_city[['Index Value']])\n",
    "    \n",
    "    # Define features and target\n",
    "    df_city['day_of_year'] = df_city.index.dayofyear\n",
    "    df_city['year'] = df_city.index.year\n",
    "    X = df_city[['day_of_year', 'year']]\n",
    "    y = df_city['Index Value']\n",
    "    \n",
    "    # Split data for training/testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    if city in cities_with_2_years_data:\n",
    "        # Apply Holt-Winters\n",
    "        seasonal_periods = min(365, len(df_city) // 2)\n",
    "        model_hw = ExponentialSmoothing(y_train, trend='add', seasonal='add', seasonal_periods=seasonal_periods).fit()\n",
    "        hw_pred = model_hw.forecast(len(X_test))\n",
    "        X_train['hw_prediction'] = model_hw.fittedvalues\n",
    "        X_test['hw_prediction'] = hw_pred\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    model_xgb = XGBRegressor(objective='reg:squarederror', n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    xgb_pred = model_xgb.predict(X_test)\n",
    "    \n",
    "    # Final prediction\n",
    "    final_prediction = xgb_pred if city in cities_with_less_than_2_years_data else (0.5 * xgb_pred + 0.5 * hw_pred)\n",
    "    \n",
    "    # Compute confidence\n",
    "    mae = mean_absolute_error(y_test, final_prediction)\n",
    "    confidence = max(0, 100 - (mae / np.mean(y_test)) * 100)\n",
    "    \n",
    "    # Store results\n",
    "    prediction_results.append({\n",
    "        \"City\": city,\n",
    "        \"Predicted AQI\": round(final_prediction[-1], 2),\n",
    "        \"Prediction Confidence\": round(confidence, 2)\n",
    "    })\n",
    "\n",
    "# Output predictions\n",
    "for result in prediction_results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'City': 'Agra', 'Future Date': '2025-02-25', 'Predicted AQI': 129.84, 'Prediction Confidence': 29.63}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load dataset\n",
    "  # Update with your file path\n",
    "\n",
    "# Convert date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Remove duplicate rows based on 'City' and 'date'\n",
    "df = df.drop_duplicates(subset=['City', 'date'])\n",
    "\n",
    "# Count unique days per city\n",
    "city_data_counts = df.groupby('City')['date'].nunique()\n",
    "\n",
    "# Categorize cities\n",
    "cities_with_2_years_data = city_data_counts[city_data_counts > 730].index.tolist()\n",
    "cities_with_less_than_2_years_data = city_data_counts[city_data_counts <= 730].index.tolist()\n",
    "\n",
    "def predict_aqi(city, future_date):\n",
    "    future_date = pd.to_datetime(future_date)\n",
    "    if city not in df['City'].unique():\n",
    "        return {\"Error\": \"City not found in dataset\"}\n",
    "    \n",
    "    df_city = df[df['City'] == city].copy()\n",
    "    df_city.set_index('date', inplace=True)\n",
    "    df_city = df_city[~df_city.index.duplicated(keep='first')]\n",
    "    df_city = df_city.asfreq('D')\n",
    "    \n",
    "    # Fill missing values using linear interpolation\n",
    "    df_city['Index Value'] = df_city['Index Value'].interpolate(method='linear')\n",
    "    \n",
    "    # Use Iterative Imputer for consecutive missing values\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    df_city[['Index Value']] = imputer.fit_transform(df_city[['Index Value']])\n",
    "    \n",
    "    # Define features and target\n",
    "    df_city['day_of_year'] = df_city.index.dayofyear\n",
    "    df_city['year'] = df_city.index.year\n",
    "    X = df_city[['day_of_year', 'year']]\n",
    "    y = df_city['Index Value']\n",
    "    \n",
    "    # Split data for training/testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    if city in cities_with_2_years_data:\n",
    "        # Apply Holt-Winters\n",
    "        seasonal_periods = min(365, len(df_city) // 2)\n",
    "        model_hw = ExponentialSmoothing(y_train, trend='add', seasonal='add', seasonal_periods=seasonal_periods).fit()\n",
    "        hw_pred = model_hw.forecast(steps=1).iloc[0]\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    model_xgb = XGBRegressor(objective='reg:squarederror', n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    future_features = pd.DataFrame({'day_of_year': [future_date.dayofyear], 'year': [future_date.year]})\n",
    "    xgb_pred = model_xgb.predict(future_features)[0]\n",
    "    \n",
    "    # Final prediction\n",
    "    final_prediction = xgb_pred if city in cities_with_less_than_2_years_data else (0.5 * xgb_pred + 0.5 * hw_pred)\n",
    "    \n",
    "    # Compute confidence\n",
    "    mae = mean_absolute_error(y_test, model_xgb.predict(X_test))\n",
    "    confidence = max(0, 100 - (mae / np.mean(y_test)) * 100)\n",
    "    \n",
    "    return {\n",
    "        \"City\": city,\n",
    "        \"Future Date\": future_date.strftime('%Y-%m-%d'),\n",
    "        \"Predicted AQI\": round(final_prediction, 2),\n",
    "        \"Prediction Confidence\": round(confidence, 2)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "city_input = input(\"Enter city name: \")\n",
    "date_input = input(\"Enter future date (YYYY-MM-DD): \")\n",
    "prediction = predict_aqi(city_input, date_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'City': 'Agra', 'Future Date': '2024-09-18', 'Predicted AQI': 101.66}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load dataset\n",
    "  # Update with your file path\n",
    "\n",
    "# Convert date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Remove duplicate rows based on 'City' and 'date'\n",
    "df = df.drop_duplicates(subset=['City', 'date'])\n",
    "\n",
    "# Count unique days per city\n",
    "city_data_counts = df.groupby('City')['date'].nunique()\n",
    "\n",
    "# Categorize cities\n",
    "cities_with_2_years_data = city_data_counts[city_data_counts > 730].index.tolist()\n",
    "cities_with_less_than_2_years_data = city_data_counts[city_data_counts <= 730].index.tolist()\n",
    "\n",
    "# Dictionary to store trained models\n",
    "models = {}\n",
    "\n",
    "for city in df['City'].unique():\n",
    "    df_city = df[df['City'] == city].copy()\n",
    "    df_city.set_index('date', inplace=True)\n",
    "    df_city = df_city[~df_city.index.duplicated(keep='first')]\n",
    "    df_city = df_city.asfreq('D')\n",
    "    \n",
    "    # Fill missing values using linear interpolation\n",
    "    df_city['Index Value'] = df_city['Index Value'].interpolate(method='linear')\n",
    "    \n",
    "    # Use Iterative Imputer for consecutive missing values\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    df_city[['Index Value']] = imputer.fit_transform(df_city[['Index Value']])\n",
    "    \n",
    "    # Define features and target\n",
    "    df_city['day_of_year'] = df_city.index.dayofyear\n",
    "    df_city['year'] = df_city.index.year\n",
    "    X = df_city[['day_of_year', 'year']]\n",
    "    y = df_city['Index Value']\n",
    "    \n",
    "    # Split data for training/testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    model_xgb = XGBRegressor(objective='reg:squarederror', n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    if city in cities_with_2_years_data:\n",
    "        seasonal_periods = min(365, len(df_city) // 2)\n",
    "        model_hw = ExponentialSmoothing(y_train, trend='add', seasonal='add', seasonal_periods=seasonal_periods).fit()\n",
    "        models[city] = (model_xgb, model_hw)\n",
    "    else:\n",
    "        models[city] = (model_xgb, None)\n",
    "\n",
    "# Save models to disk\n",
    "with open(\"aqi_prediction_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "def predict_aqi(city, future_date):\n",
    "    future_date = pd.to_datetime(future_date)\n",
    "    with open(\"aqi_prediction_models.pkl\", \"rb\") as f:\n",
    "        models = pickle.load(f)\n",
    "    \n",
    "    if city not in models:\n",
    "        return {\"Error\": \"City not found in dataset\"}\n",
    "    \n",
    "    model_xgb, model_hw = models[city]\n",
    "    future_features = pd.DataFrame({'day_of_year': [future_date.dayofyear], 'year': [future_date.year]})\n",
    "    xgb_pred = model_xgb.predict(future_features)[0]\n",
    "    \n",
    "    if model_hw:\n",
    "        hw_pred = model_hw.forecast(steps=1).iloc[0]\n",
    "        final_prediction = 0.5 * xgb_pred + 0.5 * hw_pred\n",
    "       \n",
    "    else:\n",
    "        final_prediction = xgb_pred\n",
    "    \n",
    "    return {\n",
    "        \"City\": city,\n",
    "        \"Future Date\": future_date.strftime('%Y-%m-%d'),\n",
    "        \"Predicted AQI\": round(final_prediction, 2),\n",
    "       \n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "city_input = input(\"Enter city name: \")\n",
    "date_input = input(\"Enter future date (YYYY-MM-DD): \")\n",
    "prediction = predict_aqi(city_input, date_input)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'City': 'Agra', 'Future Date': '2025-05-20', 'Predicted AQI': 121.44}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def predict_aqi(city, future_date):\n",
    "    future_date = pd.to_datetime(future_date)\n",
    "    \n",
    "    # Load the saved models\n",
    "    with open(\"aqi_prediction_models.pkl\", \"rb\") as f:\n",
    "        models = pickle.load(f)\n",
    "    \n",
    "    if city not in models:\n",
    "        return {\"Error\": \"City not found in dataset\"}\n",
    "    \n",
    "    model_xgb, model_hw = models[city]\n",
    "    future_features = pd.DataFrame({'day_of_year': [future_date.dayofyear], 'year': [future_date.year]})\n",
    "    xgb_pred = model_xgb.predict(future_features)[0]\n",
    "    \n",
    "    if model_hw:\n",
    "        hw_pred = model_hw.forecast(steps=1).iloc[0]\n",
    "        final_prediction = 0.5 * xgb_pred + 0.5 * hw_pred\n",
    "    else:\n",
    "        final_prediction = xgb_pred\n",
    "    \n",
    "    return {\n",
    "        \"City\": city,\n",
    "        \"Future Date\": future_date.strftime('%Y-%m-%d'),\n",
    "        \"Predicted AQI\": round(final_prediction, 2)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "city_input = input(\"Enter city name: \")\n",
    "date_input = input(\"Enter future date (YYYY-MM-DD): \")\n",
    "prediction = predict_aqi(city_input, date_input)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'City': 'Greater Noida', 'Future Date': '2024-06-06', 'Predicted AQI': 189.02, 'Prediction Confidence': 0.18}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load dataset\n",
    " # Update with your file path\n",
    "\n",
    "# Convert date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Remove duplicate rows based on 'City' and 'date'\n",
    "df = df.drop_duplicates(subset=['City', 'date'])\n",
    "\n",
    "# Count unique days per city\n",
    "city_data_counts = df.groupby('City')['date'].nunique()\n",
    "\n",
    "# Categorize cities\n",
    "cities_with_2_years_data = city_data_counts[city_data_counts > 730].index.tolist()\n",
    "cities_with_less_than_2_years_data = city_data_counts[city_data_counts <= 730].index.tolist()\n",
    "\n",
    "# Dictionary to store trained models\n",
    "models = {}\n",
    "errors = {}\n",
    "\n",
    "for city in df['City'].unique():\n",
    "    df_city = df[df['City'] == city].copy()\n",
    "    df_city.set_index('date', inplace=True)\n",
    "    df_city = df_city[~df_city.index.duplicated(keep='first')]\n",
    "    df_city = df_city.asfreq('D')\n",
    "    \n",
    "    # Fill missing values using linear interpolation\n",
    "    df_city['Index Value'] = df_city['Index Value'].interpolate(method='linear')\n",
    "    \n",
    "    # Use Iterative Imputer for consecutive missing values\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    df_city[['Index Value']] = imputer.fit_transform(df_city[['Index Value']])\n",
    "    \n",
    "    # Define features and target\n",
    "    df_city['day_of_year'] = df_city.index.dayofyear\n",
    "    df_city['year'] = df_city.index.year\n",
    "    X = df_city[['day_of_year', 'year']]\n",
    "    y = df_city['Index Value']\n",
    "    \n",
    "    # Split data for training/testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    model_xgb = XGBRegressor(objective='reg:squarederror', n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model_xgb.predict(X_test)\n",
    "    error_xgb = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    if city in cities_with_2_years_data:\n",
    "        seasonal_periods = min(365, len(df_city) // 2)\n",
    "        model_hw = ExponentialSmoothing(y_train, trend='add', seasonal='add', seasonal_periods=seasonal_periods).fit()\n",
    "        hw_pred = model_hw.forecast(steps=len(y_test))\n",
    "        error_hw = mean_absolute_error(y_test, hw_pred)\n",
    "        models[city] = (model_xgb, model_hw)\n",
    "        errors[city] = (error_xgb, error_hw)\n",
    "    else:\n",
    "        models[city] = (model_xgb, None)\n",
    "        errors[city] = (error_xgb, None)\n",
    "\n",
    "# Save models and errors to disk\n",
    "with open(\"aqi_prediction_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models, f)\n",
    "with open(\"aqi_prediction_errors.pkl\", \"wb\") as f:\n",
    "    pickle.dump(errors, f)\n",
    "\n",
    "def predict_aqi(city, future_date):\n",
    "    future_date = pd.to_datetime(future_date)\n",
    "    with open(\"aqi_prediction_models.pkl\", \"rb\") as f:\n",
    "        models = pickle.load(f)\n",
    "    with open(\"aqi_prediction_errors.pkl\", \"rb\") as f:\n",
    "        errors = pickle.load(f)\n",
    "    \n",
    "    if city not in models:\n",
    "        return {\"Error\": \"City not found in dataset\"}\n",
    "    \n",
    "    model_xgb, model_hw = models[city]\n",
    "    error_xgb, error_hw = errors.get(city, (None, None))\n",
    "    future_features = pd.DataFrame({'day_of_year': [future_date.dayofyear], 'year': [future_date.year]})\n",
    "    xgb_pred = model_xgb.predict(future_features)[0]\n",
    "    \n",
    "    if model_hw:\n",
    "        hw_pred = model_hw.forecast(steps=1).iloc[0]\n",
    "        final_prediction = 0.5 * xgb_pred + 0.5 * hw_pred\n",
    "        confidence = max(0, 100 * np.exp(-((error_xgb + (error_hw or 0)) / 50)))\n",
    "    else:\n",
    "        final_prediction = xgb_pred\n",
    "        confidence = max(0, 100 * np.exp(-error_xgb / 50))\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"City\": city,\n",
    "        \"Future Date\": future_date.strftime('%Y-%m-%d'),\n",
    "        \"Predicted AQI\": round(final_prediction, 2),\n",
    "        \"Prediction Confidence\": round(confidence, 2)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "city_input = input(\"Enter city name: \")\n",
    "date_input = input(\"Enter future date (YYYY-MM-DD): \")\n",
    "prediction = predict_aqi(city_input, date_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
