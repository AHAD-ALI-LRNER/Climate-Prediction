{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\ASUS\\Desktop\\Climate1\\AQI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No. Stations</th>\n",
       "      <th>Index Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53379.000000</td>\n",
       "      <td>53430.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.389348</td>\n",
       "      <td>121.662081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.049632</td>\n",
       "      <td>81.358872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       No. Stations   Index Value\n",
       "count  53379.000000  53430.000000\n",
       "mean       2.389348    121.662081\n",
       "std        3.049632     81.358872\n",
       "min        1.000000      6.000000\n",
       "25%        1.000000     63.000000\n",
       "50%        1.000000     97.000000\n",
       "75%        2.000000    156.000000\n",
       "max       28.000000    500.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anacondaa\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - loss: 0.0227\n",
      "Epoch 2/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0131\n",
      "Epoch 3/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0131\n",
      "Epoch 4/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0117\n",
      "Epoch 5/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0113\n",
      "Epoch 6/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0112\n",
      "Epoch 7/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0113\n",
      "Epoch 8/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0105\n",
      "Epoch 9/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0103\n",
      "Epoch 10/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0098\n",
      "Epoch 11/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0114\n",
      "Epoch 12/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0093\n",
      "Epoch 13/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0107\n",
      "Epoch 14/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0098\n",
      "Epoch 15/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0105\n",
      "Epoch 16/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0110\n",
      "Epoch 17/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0105\n",
      "Epoch 18/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0102\n",
      "Epoch 19/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0092\n",
      "Epoch 20/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0098\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n",
      "Predicted AQI for Agra on 2025-02-27: 105.48 (Confidence: 100.00%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def train_bilstm_model(city_name, df):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    \n",
    "    if len(city_data) <= 30:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum 31 days required.\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    city_data.loc[:, 'Scaled_AQI'] = scaler.fit_transform(city_data[['Index Value']])\n",
    "    \n",
    "    look_back = 30  # Days of past data used for prediction\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(city_data) - look_back):\n",
    "        X.append(city_data['Scaled_AQI'].iloc[i:i+look_back].values)\n",
    "        y.append(city_data['Scaled_AQI'].iloc[i+look_back])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(50, return_sequences=True), input_shape=(look_back, 1)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(50)),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=20, batch_size=16, verbose=1)\n",
    "    \n",
    "    df.loc[df['City'].str.lower() == city_name.lower(), 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    return model, scaler, look_back\n",
    "\n",
    "def predict_future_aqi(city_name, future_date, df, model, scaler, look_back):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    if len(city_data) <= look_back:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum {look_back+1} days required.\")\n",
    "    \n",
    "    city_data.loc[:, 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    last_days = city_data['Scaled_AQI'].iloc[-look_back:].values.reshape((1, look_back, 1))\n",
    "    \n",
    "    pred_scaled = model.predict(last_days)\n",
    "    pred_aqi = scaler.inverse_transform(pred_scaled.reshape(-1, 1))[0][0]\n",
    "    \n",
    "    confidence = 1 - abs(model.evaluate(last_days, pred_scaled, verbose=0))  # Simple confidence metric\n",
    "    \n",
    "    return pred_aqi, confidence\n",
    "\n",
    "# Example Usage\n",
    "city = \"Agra\"\n",
    "future_date = \"2025-02-27\"\n",
    "model, scaler, look_back = train_bilstm_model(city, df)\n",
    "pred_aqi, confidence = predict_future_aqi(city, future_date, df, model, scaler, look_back)\n",
    "print(f\"Predicted AQI for {city} on {future_date}: {pred_aqi:.2f} (Confidence: {confidence:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anacondaa\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 0.0247\n",
      "Epoch 2/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0128\n",
      "Epoch 3/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0118\n",
      "Epoch 4/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0107\n",
      "Epoch 5/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0121\n",
      "Epoch 6/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0126\n",
      "Epoch 7/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0105\n",
      "Epoch 8/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0125\n",
      "Epoch 9/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0106\n",
      "Epoch 10/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0116\n",
      "Epoch 11/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0103\n",
      "Epoch 12/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0097\n",
      "Epoch 13/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0109\n",
      "Epoch 14/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0101\n",
      "Epoch 15/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0102\n",
      "Epoch 16/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0109\n",
      "Epoch 17/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0100\n",
      "Epoch 18/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0093\n",
      "Epoch 19/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0104\n",
      "Epoch 20/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "MAPE (Mean Absolute Percentage Error): 21.88%\n",
      "Predicted AQI for Agra on 2025-02-27: 113.35 (95% CI: 113.35 - 113.35)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def train_bilstm_model(city_name, df):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    \n",
    "    if len(city_data) <= 30:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum 31 days required.\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    city_data.loc[:, 'Scaled_AQI'] = scaler.fit_transform(city_data[['Index Value']])\n",
    "    \n",
    "    look_back = 30  # Days of past data used for prediction\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(city_data) - look_back):\n",
    "        X.append(city_data['Scaled_AQI'].iloc[i:i+look_back].values)\n",
    "        y.append(city_data['Scaled_AQI'].iloc[i+look_back])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(50, return_sequences=True), input_shape=(look_back, 1)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(50)),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=20, batch_size=16, verbose=1)\n",
    "    \n",
    "    df.loc[df['City'].str.lower() == city_name.lower(), 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    return model, scaler, look_back\n",
    "\n",
    "def predict_future_aqi(city_name, future_date, df, model, scaler, look_back, n_simulations=50, compute_mape=True):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    if len(city_data) <= look_back:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum {look_back+1} days required.\")\n",
    "    \n",
    "    city_data.loc[:, 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    last_days = city_data['Scaled_AQI'].iloc[-look_back:].values.reshape((1, look_back, 1))\n",
    "    \n",
    "    predictions = np.array([model.predict(last_days) for _ in range(n_simulations)])\n",
    "    pred_scaled_mean = predictions.mean()\n",
    "    pred_scaled_std = predictions.std()\n",
    "    pred_aqi = scaler.inverse_transform([[pred_scaled_mean]])[0][0]\n",
    "    \n",
    "    lower_bound = scaler.inverse_transform([[pred_scaled_mean - 1.96 * pred_scaled_std]])[0][0]\n",
    "    upper_bound = scaler.inverse_transform([[pred_scaled_mean + 1.96 * pred_scaled_std]])[0][0]\n",
    "    \n",
    "    \n",
    "    if compute_mape:\n",
    "        actual_aqi = city_data['Index Value'].iloc[-1]\n",
    "        predicted_values = scaler.inverse_transform(predictions.mean(axis=0).reshape(-1, 1)).flatten()\n",
    "        mape = np.mean(np.abs((actual_aqi - predicted_values) / actual_aqi)) * 100\n",
    "        print(f\"MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "    \n",
    "    return pred_aqi, lower_bound, upper_bound\n",
    "joblib.dump(model, \"ml_model.pkl\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "city = \"Agra\"\n",
    "future_date = \"2025-02-27\"\n",
    "model, scaler, look_back = train_bilstm_model(city, df)\n",
    "pred_aqi, lower_bound, upper_bound = predict_future_aqi(city, future_date, df, model, scaler, look_back)\n",
    "print(f\"Predicted AQI for {city} on {future_date}: {pred_aqi:.2f} (95% CI: {lower_bound:.2f} - {upper_bound:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "modell = load_model(\"model.h5\", custom_objects={\"mse\": MeanSquaredError()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import plotly.express as px\n",
    "\n",
    "# Load Model\n",
    "model = joblib.load(\"ml_model.pkl\")\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"🔍 ML Model Prediction Dashboard\")\n",
    "\n",
    "st.sidebar.header(\"Upload CSV Data\")\n",
    "uploaded_file = st.sidebar.file_uploader(\"Upload CSV\", type=[\"csv\"])\n",
    "\n",
    "if uploaded_file:\n",
    "    df = pd.read_csv(uploaded_file)\n",
    "    st.write(\"📊 **Uploaded Data:**\", df.head())\n",
    "\n",
    "    if st.button(\"Make Predictions\"):\n",
    "        predictions = model.predict(df)\n",
    "        df[\"Prediction\"] = predictions\n",
    "        st.write(\"✅ **Predictions:**\", df)\n",
    "\n",
    "        # Visualization\n",
    "        fig = px.scatter(df, x=df.index, y=\"Prediction\", title=\"Predictions Over Time\")\n",
    "        st.plotly_chart(fig)\n",
    "\n",
    "st.sidebar.markdown(\"### Built with ❤️ using Streamlit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>City</th>\n",
       "      <th>No. Stations</th>\n",
       "      <th>Air Quality</th>\n",
       "      <th>Index Value</th>\n",
       "      <th>Prominent Pollutant</th>\n",
       "      <th>Region</th>\n",
       "      <th>Scaled_AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>87</td>\n",
       "      <td>CO</td>\n",
       "      <td>Eastern Coastal Region</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Varanasi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>157</td>\n",
       "      <td>PM10</td>\n",
       "      <td>Indo-Gangetic Region</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>189</td>\n",
       "      <td>PM2.5</td>\n",
       "      <td>Tropical wet &amp; dry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Agra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>179</td>\n",
       "      <td>PM10</td>\n",
       "      <td>Indo-Gangetic Region</td>\n",
       "      <td>0.33264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-02</td>\n",
       "      <td>Varanasi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>156</td>\n",
       "      <td>PM10</td>\n",
       "      <td>Indo-Gangetic Region</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date       City  No. Stations   Air Quality  Index Value  \\\n",
       "0 2015-05-01    Chennai           NaN  Satisfactory           87   \n",
       "1 2015-05-01   Varanasi           NaN      Moderate          157   \n",
       "2 2015-05-01  Hyderabad           NaN      Moderate          189   \n",
       "3 2015-05-01       Agra           NaN      Moderate          179   \n",
       "4 2015-05-02   Varanasi           NaN      Moderate          156   \n",
       "\n",
       "  Prominent Pollutant                  Region  Scaled_AQI  \n",
       "0                  CO  Eastern Coastal Region         NaN  \n",
       "1                PM10    Indo-Gangetic Region         NaN  \n",
       "2               PM2.5      Tropical wet & dry         NaN  \n",
       "3                PM10    Indo-Gangetic Region     0.33264  \n",
       "4                PM10    Indo-Gangetic Region         NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... | lstm_u... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-0.008999\u001b[39m | \u001b[39m11.0     \u001b[39m | \u001b[39m0.2901   \u001b[39m | \u001b[39m24.64    \u001b[39m | \u001b[39m0.003033 \u001b[39m | \u001b[39m36.99    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m-0.008876\u001b[39m | \u001b[35m9.248    \u001b[39m | \u001b[35m0.1116   \u001b[39m | \u001b[35m27.32    \u001b[39m | \u001b[35m0.003045 \u001b[39m | \u001b[35m54.66    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-0.009549\u001b[39m | \u001b[39m8.165    \u001b[39m | \u001b[39m0.294    \u001b[39m | \u001b[39m26.65    \u001b[39m | \u001b[39m0.00114  \u001b[39m | \u001b[39m37.82    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-0.009116\u001b[39m | \u001b[39m9.116    \u001b[39m | \u001b[39m0.2138   \u001b[39m | \u001b[39m27.36    \u001b[39m | \u001b[39m0.002957 \u001b[39m | \u001b[39m54.98    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-0.00934 \u001b[39m | \u001b[39m9.456    \u001b[39m | \u001b[39m0.1276   \u001b[39m | \u001b[39m27.15    \u001b[39m | \u001b[39m0.0003579\u001b[39m | \u001b[39m54.75    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-0.01012 \u001b[39m | \u001b[39m12.67    \u001b[39m | \u001b[39m0.1096   \u001b[39m | \u001b[39m10.72    \u001b[39m | \u001b[39m0.0006122\u001b[39m | \u001b[39m39.46    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-0.009417\u001b[39m | \u001b[39m11.04    \u001b[39m | \u001b[39m0.2279   \u001b[39m | \u001b[39m24.97    \u001b[39m | \u001b[39m0.001347 \u001b[39m | \u001b[39m37.14    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-0.009179\u001b[39m | \u001b[39m9.379    \u001b[39m | \u001b[39m0.2048   \u001b[39m | \u001b[39m27.57    \u001b[39m | \u001b[39m0.001772 \u001b[39m | \u001b[39m55.21    \u001b[39m |\n",
      "=====================================================================================\n",
      "Epoch 1/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - loss: 0.0193\n",
      "Epoch 2/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0120\n",
      "Epoch 3/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0112\n",
      "Epoch 4/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0096\n",
      "Epoch 5/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0120\n",
      "Epoch 6/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0106\n",
      "Epoch 7/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0114\n",
      "Epoch 8/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0110\n",
      "Epoch 9/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0108\n",
      "Epoch 10/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0088\n",
      "Epoch 11/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0095\n",
      "Epoch 12/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0092\n",
      "Epoch 13/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0111\n",
      "Epoch 14/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0098\n",
      "Epoch 15/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0099\n",
      "Epoch 16/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0098\n",
      "Epoch 17/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0092\n",
      "Epoch 18/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0100\n",
      "Epoch 19/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0086\n",
      "Epoch 20/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0095\n",
      "Epoch 21/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0101\n",
      "Epoch 22/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0100\n",
      "Epoch 23/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0094\n",
      "Epoch 24/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0097\n",
      "Epoch 25/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0094\n",
      "Epoch 26/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 0.0089\n",
      "Epoch 27/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0098\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAPE (Mean Absolute Percentage Error): 9.28%\n",
      "Predicted AQI for Agra on 2025-03-15: 101.63 (95% CI: 101.63 - 101.63)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def train_bilstm_model(city_name, df):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    if len(city_data) <= 30:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum 31 days required.\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    city_data['Scaled_AQI'] = scaler.fit_transform(city_data[['Index Value']])\n",
    "    \n",
    "    look_back = 30  # Days of past data used for prediction\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(city_data) - look_back):\n",
    "        X.append(city_data['Scaled_AQI'].iloc[i:i+look_back].values)\n",
    "        y.append(city_data['Scaled_AQI'].iloc[i+look_back])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    def bilstm_optimize(lstm_units, dropout_rate, learning_rate, epochs, batch_size):\n",
    "        model = Sequential([\n",
    "            Input(shape=(look_back, 1)),\n",
    "            Bidirectional(LSTM(int(lstm_units), return_sequences=True)),\n",
    "            Dropout(dropout_rate),\n",
    "            Bidirectional(LSTM(int(lstm_units))),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "        model.fit(X, y, epochs=int(epochs), batch_size=int(batch_size), verbose=0)\n",
    "        loss = model.evaluate(X, y, verbose=0)\n",
    "        return -loss  # Negative MSE for maximization\n",
    "    \n",
    "    pbounds = {\n",
    "        'lstm_units': (32, 64),  # Reduced upper bound\n",
    "        'dropout_rate': (0.1, 0.3),  # Reduced range\n",
    "        'learning_rate': (1e-4, 5e-3),  # Reduced upper bound\n",
    "        'epochs': (10, 30),  # Reduced max epochs\n",
    "        'batch_size': (8, 16)  # Reduced max batch size\n",
    "    }\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=bilstm_optimize, pbounds=pbounds, random_state=42)\n",
    "    optimizer.maximize(init_points=3, n_iter=5)  # Reduced iterations\n",
    "    \n",
    "    best_params = optimizer.max['params']\n",
    "    model = Sequential([\n",
    "        Input(shape=(look_back, 1)),\n",
    "        Bidirectional(LSTM(int(best_params['lstm_units']), return_sequences=True)),\n",
    "        Dropout(best_params['dropout_rate']),\n",
    "        Bidirectional(LSTM(int(best_params['lstm_units']))),\n",
    "        Dropout(best_params['dropout_rate']),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "    model.fit(X, y, epochs=int(best_params['epochs']), batch_size=int(best_params['batch_size']), verbose=1)\n",
    "    \n",
    "    df.loc[df['City'].str.lower() == city_name.lower(), 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    return model, scaler, look_back\n",
    "\n",
    "def predict_future_aqi(city_name, future_date, df, model, scaler, look_back, n_simulations=30, compute_mape=True):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    if len(city_data) <= look_back:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum {look_back+1} days required.\")\n",
    "    \n",
    "    city_data['Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    last_days = city_data['Scaled_AQI'].iloc[-look_back:].values.reshape((1, look_back, 1))\n",
    "    \n",
    "    predictions = np.array([model.predict(last_days) for _ in range(n_simulations)])\n",
    "    pred_scaled_mean = predictions.mean()\n",
    "    pred_scaled_std = predictions.std()\n",
    "    pred_aqi = scaler.inverse_transform([[pred_scaled_mean]])[0][0]\n",
    "    \n",
    "    lower_bound = scaler.inverse_transform([[pred_scaled_mean - 1.96 * pred_scaled_std]])[0][0]\n",
    "    upper_bound = scaler.inverse_transform([[pred_scaled_mean + 1.96 * pred_scaled_std]])[0][0]\n",
    "    \n",
    "    if compute_mape:\n",
    "        actual_aqi = city_data['Index Value'].iloc[-1]\n",
    "        predicted_values = scaler.inverse_transform(predictions.mean(axis=0).reshape(-1, 1)).flatten()\n",
    "        mape = np.mean(np.abs((actual_aqi - predicted_values) / actual_aqi)) * 100\n",
    "        print(f\"MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "    \n",
    "    return pred_aqi, lower_bound, upper_bound\n",
    "\n",
    "# Example Usage\n",
    "city = \"Agra\"\n",
    "future_date = \"2025-03-15\"\n",
    "model, scaler, look_back = train_bilstm_model(city, df)\n",
    "pred_aqi, lower_bound, upper_bound = predict_future_aqi(city, future_date, df, model, scaler, look_back)\n",
    "print(f\"Predicted AQI for {city} on {future_date}: {pred_aqi:.2f} (95% CI: {lower_bound:.2f} - {upper_bound:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Example: Assume you have trained a BiLSTM model\n",
    "model = Sequential()  # Your actual model should be defined and trained\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"model.h5\")\n",
    "print(\"✅ Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
