{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\ASUS\\Desktop\\Climate1\\AQI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No. Stations</th>\n",
       "      <th>Index Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53379.000000</td>\n",
       "      <td>53430.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.389348</td>\n",
       "      <td>121.662081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.049632</td>\n",
       "      <td>81.358872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       No. Stations   Index Value\n",
       "count  53379.000000  53430.000000\n",
       "mean       2.389348    121.662081\n",
       "std        3.049632     81.358872\n",
       "min        1.000000      6.000000\n",
       "25%        1.000000     63.000000\n",
       "50%        1.000000     97.000000\n",
       "75%        2.000000    156.000000\n",
       "max       28.000000    500.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anacondaa\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - loss: 0.0227\n",
      "Epoch 2/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0131\n",
      "Epoch 3/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0131\n",
      "Epoch 4/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0117\n",
      "Epoch 5/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0113\n",
      "Epoch 6/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0112\n",
      "Epoch 7/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0113\n",
      "Epoch 8/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0105\n",
      "Epoch 9/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0103\n",
      "Epoch 10/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0098\n",
      "Epoch 11/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0114\n",
      "Epoch 12/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0093\n",
      "Epoch 13/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0107\n",
      "Epoch 14/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0098\n",
      "Epoch 15/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0105\n",
      "Epoch 16/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0110\n",
      "Epoch 17/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0105\n",
      "Epoch 18/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0102\n",
      "Epoch 19/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0092\n",
      "Epoch 20/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0098\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n",
      "Predicted AQI for Agra on 2025-02-27: 105.48 (Confidence: 100.00%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def train_bilstm_model(city_name, df):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    \n",
    "    if len(city_data) <= 30:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum 31 days required.\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    city_data.loc[:, 'Scaled_AQI'] = scaler.fit_transform(city_data[['Index Value']])\n",
    "    \n",
    "    look_back = 30  # Days of past data used for prediction\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(city_data) - look_back):\n",
    "        X.append(city_data['Scaled_AQI'].iloc[i:i+look_back].values)\n",
    "        y.append(city_data['Scaled_AQI'].iloc[i+look_back])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(50, return_sequences=True), input_shape=(look_back, 1)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(50)),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=20, batch_size=16, verbose=1)\n",
    "    \n",
    "    df.loc[df['City'].str.lower() == city_name.lower(), 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    return model, scaler, look_back\n",
    "\n",
    "def predict_future_aqi(city_name, future_date, df, model, scaler, look_back):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    if len(city_data) <= look_back:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum {look_back+1} days required.\")\n",
    "    \n",
    "    city_data.loc[:, 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    last_days = city_data['Scaled_AQI'].iloc[-look_back:].values.reshape((1, look_back, 1))\n",
    "    \n",
    "    pred_scaled = model.predict(last_days)\n",
    "    pred_aqi = scaler.inverse_transform(pred_scaled.reshape(-1, 1))[0][0]\n",
    "    \n",
    "    confidence = 1 - abs(model.evaluate(last_days, pred_scaled, verbose=0))  # Simple confidence metric\n",
    "    \n",
    "    return pred_aqi, confidence\n",
    "\n",
    "# Example Usage\n",
    "city = \"Agra\"\n",
    "future_date = \"2025-02-27\"\n",
    "model, scaler, look_back = train_bilstm_model(city, df)\n",
    "pred_aqi, confidence = predict_future_aqi(city, future_date, df, model, scaler, look_back)\n",
    "print(f\"Predicted AQI for {city} on {future_date}: {pred_aqi:.2f} (Confidence: {confidence:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anacondaa\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 0.0247\n",
      "Epoch 2/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0128\n",
      "Epoch 3/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0118\n",
      "Epoch 4/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0107\n",
      "Epoch 5/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0121\n",
      "Epoch 6/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0126\n",
      "Epoch 7/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0105\n",
      "Epoch 8/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0125\n",
      "Epoch 9/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0106\n",
      "Epoch 10/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0116\n",
      "Epoch 11/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0103\n",
      "Epoch 12/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0097\n",
      "Epoch 13/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0109\n",
      "Epoch 14/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0101\n",
      "Epoch 15/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0102\n",
      "Epoch 16/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0109\n",
      "Epoch 17/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0100\n",
      "Epoch 18/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0093\n",
      "Epoch 19/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0104\n",
      "Epoch 20/20\n",
      "\u001b[1m181/181\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "MAPE (Mean Absolute Percentage Error): 21.88%\n",
      "Predicted AQI for Agra on 2025-02-27: 113.35 (95% CI: 113.35 - 113.35)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def train_bilstm_model(city_name, df):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    \n",
    "    if len(city_data) <= 30:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum 31 days required.\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    city_data.loc[:, 'Scaled_AQI'] = scaler.fit_transform(city_data[['Index Value']])\n",
    "    \n",
    "    look_back = 30  # Days of past data used for prediction\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(city_data) - look_back):\n",
    "        X.append(city_data['Scaled_AQI'].iloc[i:i+look_back].values)\n",
    "        y.append(city_data['Scaled_AQI'].iloc[i+look_back])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(50, return_sequences=True), input_shape=(look_back, 1)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(50)),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=20, batch_size=16, verbose=1)\n",
    "    \n",
    "    df.loc[df['City'].str.lower() == city_name.lower(), 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    return model, scaler, look_back\n",
    "\n",
    "def predict_future_aqi(city_name, future_date, df, model, scaler, look_back, n_simulations=50, compute_mape=True):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    if len(city_data) <= look_back:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum {look_back+1} days required.\")\n",
    "    \n",
    "    city_data.loc[:, 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    last_days = city_data['Scaled_AQI'].iloc[-look_back:].values.reshape((1, look_back, 1))\n",
    "    \n",
    "    predictions = np.array([model.predict(last_days) for _ in range(n_simulations)])\n",
    "    pred_scaled_mean = predictions.mean()\n",
    "    pred_scaled_std = predictions.std()\n",
    "    pred_aqi = scaler.inverse_transform([[pred_scaled_mean]])[0][0]\n",
    "    \n",
    "    lower_bound = scaler.inverse_transform([[pred_scaled_mean - 1.96 * pred_scaled_std]])[0][0]\n",
    "    upper_bound = scaler.inverse_transform([[pred_scaled_mean + 1.96 * pred_scaled_std]])[0][0]\n",
    "    \n",
    "    \n",
    "    if compute_mape:\n",
    "        actual_aqi = city_data['Index Value'].iloc[-1]\n",
    "        predicted_values = scaler.inverse_transform(predictions.mean(axis=0).reshape(-1, 1)).flatten()\n",
    "        mape = np.mean(np.abs((actual_aqi - predicted_values) / actual_aqi)) * 100\n",
    "        print(f\"MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "    \n",
    "    return pred_aqi, lower_bound, upper_bound\n",
    "joblib.dump(model, \"ml_model.pkl\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "city = \"Agra\"\n",
    "future_date = \"2025-02-27\"\n",
    "model, scaler, look_back = train_bilstm_model(city, df)\n",
    "pred_aqi, lower_bound, upper_bound = predict_future_aqi(city, future_date, df, model, scaler, look_back)\n",
    "print(f\"Predicted AQI for {city} on {future_date}: {pred_aqi:.2f} (95% CI: {lower_bound:.2f} - {upper_bound:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "modell = load_model(\"model.h5\", custom_objects={\"mse\": MeanSquaredError()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import plotly.express as px\n",
    "\n",
    "# Load Model\n",
    "model = joblib.load(\"ml_model.pkl\")\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"ğŸ” ML Model Prediction Dashboard\")\n",
    "\n",
    "st.sidebar.header(\"Upload CSV Data\")\n",
    "uploaded_file = st.sidebar.file_uploader(\"Upload CSV\", type=[\"csv\"])\n",
    "\n",
    "if uploaded_file:\n",
    "    df = pd.read_csv(uploaded_file)\n",
    "    st.write(\"ğŸ“Š **Uploaded Data:**\", df.head())\n",
    "\n",
    "    if st.button(\"Make Predictions\"):\n",
    "        predictions = model.predict(df)\n",
    "        df[\"Prediction\"] = predictions\n",
    "        st.write(\"âœ… **Predictions:**\", df)\n",
    "\n",
    "        # Visualization\n",
    "        fig = px.scatter(df, x=df.index, y=\"Prediction\", title=\"Predictions Over Time\")\n",
    "        st.plotly_chart(fig)\n",
    "\n",
    "st.sidebar.markdown(\"### Built with â¤ï¸ using Streamlit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>City</th>\n",
       "      <th>No. Stations</th>\n",
       "      <th>Air Quality</th>\n",
       "      <th>Index Value</th>\n",
       "      <th>Prominent Pollutant</th>\n",
       "      <th>Region</th>\n",
       "      <th>Scaled_AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satisfactory</td>\n",
       "      <td>87</td>\n",
       "      <td>CO</td>\n",
       "      <td>Eastern Coastal Region</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Varanasi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>157</td>\n",
       "      <td>PM10</td>\n",
       "      <td>Indo-Gangetic Region</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>189</td>\n",
       "      <td>PM2.5</td>\n",
       "      <td>Tropical wet &amp; dry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Agra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>179</td>\n",
       "      <td>PM10</td>\n",
       "      <td>Indo-Gangetic Region</td>\n",
       "      <td>0.33264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-02</td>\n",
       "      <td>Varanasi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>156</td>\n",
       "      <td>PM10</td>\n",
       "      <td>Indo-Gangetic Region</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date       City  No. Stations   Air Quality  Index Value  \\\n",
       "0 2015-05-01    Chennai           NaN  Satisfactory           87   \n",
       "1 2015-05-01   Varanasi           NaN      Moderate          157   \n",
       "2 2015-05-01  Hyderabad           NaN      Moderate          189   \n",
       "3 2015-05-01       Agra           NaN      Moderate          179   \n",
       "4 2015-05-02   Varanasi           NaN      Moderate          156   \n",
       "\n",
       "  Prominent Pollutant                  Region  Scaled_AQI  \n",
       "0                  CO  Eastern Coastal Region         NaN  \n",
       "1                PM10    Indo-Gangetic Region         NaN  \n",
       "2               PM2.5      Tropical wet & dry         NaN  \n",
       "3                PM10    Indo-Gangetic Region     0.33264  \n",
       "4                PM10    Indo-Gangetic Region         NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... | lstm_u... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-0.008999\u001b[39m | \u001b[39m11.0     \u001b[39m | \u001b[39m0.2901   \u001b[39m | \u001b[39m24.64    \u001b[39m | \u001b[39m0.003033 \u001b[39m | \u001b[39m36.99    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m-0.008876\u001b[39m | \u001b[35m9.248    \u001b[39m | \u001b[35m0.1116   \u001b[39m | \u001b[35m27.32    \u001b[39m | \u001b[35m0.003045 \u001b[39m | \u001b[35m54.66    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-0.009549\u001b[39m | \u001b[39m8.165    \u001b[39m | \u001b[39m0.294    \u001b[39m | \u001b[39m26.65    \u001b[39m | \u001b[39m0.00114  \u001b[39m | \u001b[39m37.82    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-0.009116\u001b[39m | \u001b[39m9.116    \u001b[39m | \u001b[39m0.2138   \u001b[39m | \u001b[39m27.36    \u001b[39m | \u001b[39m0.002957 \u001b[39m | \u001b[39m54.98    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-0.00934 \u001b[39m | \u001b[39m9.456    \u001b[39m | \u001b[39m0.1276   \u001b[39m | \u001b[39m27.15    \u001b[39m | \u001b[39m0.0003579\u001b[39m | \u001b[39m54.75    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-0.01012 \u001b[39m | \u001b[39m12.67    \u001b[39m | \u001b[39m0.1096   \u001b[39m | \u001b[39m10.72    \u001b[39m | \u001b[39m0.0006122\u001b[39m | \u001b[39m39.46    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-0.009417\u001b[39m | \u001b[39m11.04    \u001b[39m | \u001b[39m0.2279   \u001b[39m | \u001b[39m24.97    \u001b[39m | \u001b[39m0.001347 \u001b[39m | \u001b[39m37.14    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-0.009179\u001b[39m | \u001b[39m9.379    \u001b[39m | \u001b[39m0.2048   \u001b[39m | \u001b[39m27.57    \u001b[39m | \u001b[39m0.001772 \u001b[39m | \u001b[39m55.21    \u001b[39m |\n",
      "=====================================================================================\n",
      "Epoch 1/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - loss: 0.0193\n",
      "Epoch 2/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0120\n",
      "Epoch 3/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0112\n",
      "Epoch 4/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0096\n",
      "Epoch 5/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0120\n",
      "Epoch 6/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0106\n",
      "Epoch 7/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0114\n",
      "Epoch 8/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0110\n",
      "Epoch 9/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0108\n",
      "Epoch 10/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0088\n",
      "Epoch 11/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0095\n",
      "Epoch 12/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0092\n",
      "Epoch 13/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0111\n",
      "Epoch 14/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0098\n",
      "Epoch 15/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0099\n",
      "Epoch 16/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0098\n",
      "Epoch 17/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0092\n",
      "Epoch 18/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0100\n",
      "Epoch 19/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0086\n",
      "Epoch 20/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0095\n",
      "Epoch 21/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0101\n",
      "Epoch 22/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0100\n",
      "Epoch 23/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0094\n",
      "Epoch 24/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0097\n",
      "Epoch 25/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0094\n",
      "Epoch 26/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 0.0089\n",
      "Epoch 27/27\n",
      "\u001b[1m322/322\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0098\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "MAPE (Mean Absolute Percentage Error): 9.28%\n",
      "Predicted AQI for Agra on 2025-03-15: 101.63 (95% CI: 101.63 - 101.63)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def train_bilstm_model(city_name, df):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    if len(city_data) <= 30:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum 31 days required.\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    city_data['Scaled_AQI'] = scaler.fit_transform(city_data[['Index Value']])\n",
    "    \n",
    "    look_back = 30  # Days of past data used for prediction\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(city_data) - look_back):\n",
    "        X.append(city_data['Scaled_AQI'].iloc[i:i+look_back].values)\n",
    "        y.append(city_data['Scaled_AQI'].iloc[i+look_back])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "    def bilstm_optimize(lstm_units, dropout_rate, learning_rate, epochs, batch_size):\n",
    "        model = Sequential([\n",
    "            Input(shape=(look_back, 1)),\n",
    "            Bidirectional(LSTM(int(lstm_units), return_sequences=True)),\n",
    "            Dropout(dropout_rate),\n",
    "            Bidirectional(LSTM(int(lstm_units))),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "        model.fit(X, y, epochs=int(epochs), batch_size=int(batch_size), verbose=0)\n",
    "        loss = model.evaluate(X, y, verbose=0)\n",
    "        return -loss  # Negative MSE for maximization\n",
    "    \n",
    "    pbounds = {\n",
    "        'lstm_units': (32, 64),  # Reduced upper bound\n",
    "        'dropout_rate': (0.1, 0.3),  # Reduced range\n",
    "        'learning_rate': (1e-4, 5e-3),  # Reduced upper bound\n",
    "        'epochs': (10, 30),  # Reduced max epochs\n",
    "        'batch_size': (8, 16)  # Reduced max batch size\n",
    "    }\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=bilstm_optimize, pbounds=pbounds, random_state=42)\n",
    "    optimizer.maximize(init_points=3, n_iter=5)  # Reduced iterations\n",
    "    \n",
    "    best_params = optimizer.max['params']\n",
    "    model = Sequential([\n",
    "        Input(shape=(look_back, 1)),\n",
    "        Bidirectional(LSTM(int(best_params['lstm_units']), return_sequences=True)),\n",
    "        Dropout(best_params['dropout_rate']),\n",
    "        Bidirectional(LSTM(int(best_params['lstm_units']))),\n",
    "        Dropout(best_params['dropout_rate']),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "    model.fit(X, y, epochs=int(best_params['epochs']), batch_size=int(best_params['batch_size']), verbose=1)\n",
    "    \n",
    "    df.loc[df['City'].str.lower() == city_name.lower(), 'Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    return model, scaler, look_back\n",
    "\n",
    "def predict_future_aqi(city_name, future_date, df, model, scaler, look_back, n_simulations=30, compute_mape=True):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='date')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        raise ValueError(f\"No data available for {city_name}. Check spelling or dataset.\")\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['Index Value'])\n",
    "    \n",
    "    if len(city_data) <= look_back:\n",
    "        raise ValueError(f\"Not enough historical data for {city_name}. Minimum {look_back+1} days required.\")\n",
    "    \n",
    "    city_data['Scaled_AQI'] = scaler.transform(city_data[['Index Value']])\n",
    "    last_days = city_data['Scaled_AQI'].iloc[-look_back:].values.reshape((1, look_back, 1))\n",
    "    \n",
    "    predictions = np.array([model.predict(last_days) for _ in range(n_simulations)])\n",
    "    pred_scaled_mean = predictions.mean()\n",
    "    pred_scaled_std = predictions.std()\n",
    "    pred_aqi = scaler.inverse_transform([[pred_scaled_mean]])[0][0]\n",
    "    \n",
    "    lower_bound = scaler.inverse_transform([[pred_scaled_mean - 1.96 * pred_scaled_std]])[0][0]\n",
    "    upper_bound = scaler.inverse_transform([[pred_scaled_mean + 1.96 * pred_scaled_std]])[0][0]\n",
    "    \n",
    "    if compute_mape:\n",
    "        actual_aqi = city_data['Index Value'].iloc[-1]\n",
    "        predicted_values = scaler.inverse_transform(predictions.mean(axis=0).reshape(-1, 1)).flatten()\n",
    "        mape = np.mean(np.abs((actual_aqi - predicted_values) / actual_aqi)) * 100\n",
    "        print(f\"MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "    \n",
    "    return pred_aqi, lower_bound, upper_bound\n",
    "\n",
    "# Example Usage\n",
    "city = \"Agra\"\n",
    "future_date = \"2025-03-15\"\n",
    "model, scaler, look_back = train_bilstm_model(city, df)\n",
    "pred_aqi, lower_bound, upper_bound = predict_future_aqi(city, future_date, df, model, scaler, look_back)\n",
    "print(f\"Predicted AQI for {city} on {future_date}: {pred_aqi:.2f} (95% CI: {lower_bound:.2f} - {upper_bound:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Example: Assume you have trained a BiLSTM model\n",
    "model = Sequential()  # Your actual model should be defined and trained\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"model.h5\")\n",
    "print(\"âœ… Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
