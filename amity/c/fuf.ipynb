{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import datetime\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Load the LSTM model\n",
    "LSTM_MODEL_PATH = r'E:\\Climate_project\\amity\\gulam_dataset\\bilstm_pm25_model.h5'\n",
    "HOLT_WINTERS_MODEL_PATH = r\"E:\\Climate_project\\amity\\gulam_dataset\\holts_winter_model.pkl\"\n",
    "\n",
    "\n",
    "# Load LSTM Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# Define custom objects\n",
    "custom_objects = {\"mse\": MeanSquaredError()}\n",
    "\n",
    "# Load LSTM Model with custom objects\n",
    "lstm_model = tf.keras.models.load_model(LSTM_MODEL_PATH, custom_objects=custom_objects)\n",
    "\n",
    "\n",
    "# Load Holt-Winters Model\n",
    "holt_winters_model = joblib.load(HOLT_WINTERS_MODEL_PATH)\n",
    "\n",
    "# Function to preprocess input for LSTM model\n",
    "def prepare_lstm_input(date):\n",
    "    \"\"\"\n",
    "    Convert the future date into a numerical format suitable for LSTM model input.\n",
    "    \"\"\"\n",
    "    return np.array([[pd.to_datetime(date).toordinal()]])\n",
    "\n",
    "# Function to predict AQI using LSTM\n",
    "def predict_with_lstm(future_date):\n",
    "    \"\"\"\n",
    "    Predict AQI using the LSTM model.\n",
    "    \"\"\"\n",
    "    input_data = prepare_lstm_input(future_date)\n",
    "    prediction = lstm_model.predict(input_data)\n",
    "    return max(0, prediction[0][0])  # Ensure AQI is non-negative\n",
    "\n",
    "# Function to predict AQI using Holt-Winters\n",
    "def predict_with_holt_winters(future_date):\n",
    "    \"\"\"\n",
    "    Predict AQI using the Holt-Winters model.\n",
    "    \"\"\"\n",
    "    future_steps = (pd.to_datetime(future_date) - pd.to_datetime(\"today\")).days\n",
    "    if future_steps < 0:\n",
    "        return \"âš ï¸ Date must be in the future!\"\n",
    "    \n",
    "    forecast = holt_winters_model.forecast(steps=future_steps)\n",
    "    return max(0, forecast.iloc[-1])  # Ensure AQI is non-negative\n",
    "\n",
    "# Main function for Gradio UI\n",
    "def predict_aqi(model_choice, city_name, future_date):\n",
    "    \"\"\"\n",
    "    Predicts AQI based on the selected model.\n",
    "    \"\"\"\n",
    "    if model_choice == \"LSTM\":\n",
    "        predicted_aqi = predict_with_lstm(future_date)\n",
    "    elif model_choice == \"Holt-Winters\":\n",
    "        predicted_aqi = predict_with_holt_winters(future_date)\n",
    "    else:\n",
    "        return \"âš ï¸ Invalid model choice!\", None\n",
    "\n",
    "    # Generate Plot\n",
    "    dates = pd.date_range(start=pd.to_datetime(\"today\"), periods=10, freq='D')\n",
    "    values = [predict_with_lstm(d) if model_choice == \"LSTM\" else predict_with_holt_winters(d) for d in dates]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(dates, values, marker='o', linestyle='-', label=f\"{model_choice} AQI Prediction\", color=\"blue\")\n",
    "    plt.axvline(pd.to_datetime(future_date), color='red', linestyle='dashed', label=\"Selected Date\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"AQI Index Value\")\n",
    "    plt.title(f\"AQI Forecast for {city_name} ({model_choice})\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Return Prediction & Plot\n",
    "    return f\"ğŸ“ˆ Predicted AQI for {city_name} on {future_date}: {round(predicted_aqi, 2)}\", plt\n",
    "\n",
    "# Define Gradio UI\n",
    "interface = gr.Interface(\n",
    "    fn=predict_aqi,\n",
    "    inputs=[\n",
    "        gr.Radio([\"LSTM\", \"Holt-Winters\"], label=\"Select Model\"),\n",
    "        gr.Textbox(label=\"Enter City Name\", placeholder=\"E.g., New York\"),\n",
    "        gr.Textbox(label=\"Enter Future Date (YYYY-MM-DD)\", placeholder=\"E.g., 2025-03-01\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Prediction Output\"),\n",
    "        gr.Plot(label=\"AQI Forecast Graph\"),\n",
    "    ],\n",
    "    title=\"ğŸŒ AQI Prediction Dashboard (LSTM & Holt-Winters)\",\n",
    "    description=\"Choose an AI model (LSTM or Holt-Winters) to predict AQI based on historical data.\",\n",
    ")\n",
    "\n",
    "# Launch Gradio App\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch(share=False)  # Runs locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\gradio\\blocks.py\", line 2108, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\gradio\\blocks.py\", line 1655, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\gradio\\utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_36588\\1689473461.py\", line 48, in gradio_predict\n",
      "    df = pd.read_csv(\"sorted_aqi_hourly_dataset.csv\")\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"d:\\Anacondaa\\Lib\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'sorted_aqi_hourly_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gradio as gr\n",
    "\n",
    "def load_model_and_scaler():\n",
    "    model = tf.keras.models.load_model(r\"E:\\Climate_project\\amity\\gulam_dataset\\bilstm_pm25_model.h5\")\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    print(\"Model and scaler loaded successfully!\")\n",
    "    return model, scaler\n",
    "\n",
    "def predict_future_pm25(city_name, df, model, scaler, look_back, future_date):\n",
    "    city_data = df[df['City'].str.lower() == city_name.lower()].sort_values(by='Datetime')\n",
    "    \n",
    "    if city_data.empty:\n",
    "        return f\"No data available for {city_name}. Check spelling or dataset.\"\n",
    "    \n",
    "    city_data = city_data.dropna(subset=['PM2.5'])\n",
    "    \n",
    "    if len(city_data) <= look_back:\n",
    "        return f\"Not enough historical data for {city_name}. Minimum {look_back+1} days required.\"\n",
    "    \n",
    "    city_data['Scaled_PM2.5'] = scaler.transform(city_data[['PM2.5']])\n",
    "    last_days = city_data['Scaled_PM2.5'].iloc[-look_back:].values.reshape((1, look_back, 1))\n",
    "    \n",
    "    future_date = datetime.strptime(future_date, \"%Y-%m-%d\")\n",
    "    last_known_date = city_data['Datetime'].max()\n",
    "    \n",
    "    if future_date <= last_known_date:\n",
    "        return \"Future date must be beyond the last recorded date in the dataset.\"\n",
    "    \n",
    "    days_ahead = (future_date - last_known_date).days\n",
    "    \n",
    "    for _ in range(days_ahead):\n",
    "        pred_scaled = model.predict(last_days)[0, 0]\n",
    "        last_days = np.roll(last_days, -1)\n",
    "        last_days[0, -1, 0] = pred_scaled\n",
    "    \n",
    "    pred_pm25 = scaler.inverse_transform([[pred_scaled]])[0][0]\n",
    "    \n",
    "    return round(pred_pm25, 2)\n",
    "\n",
    "def gradio_predict(city_name, future_date):\n",
    "    df = pd.read_csv(\"sorted_aqi_hourly_dataset.csv\")\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    look_back = 30  # Ensure consistency with training\n",
    "    \n",
    "    model, scaler = load_model_and_scaler()\n",
    "    pred_pm25 = predict_future_pm25(city_name, df, model, scaler, look_back, future_date)\n",
    "    \n",
    "    future_actual = df[(df['City'].str.lower() == city_name.lower()) & (df['Datetime'] == future_date)]\n",
    "    if not future_actual.empty:\n",
    "        actual_pm25 = future_actual['PM2.5'].values[0]\n",
    "        mse = mean_squared_error([actual_pm25], [pred_pm25])\n",
    "        return f\"Predicted PM2.5 for {city_name} on {future_date}: {pred_pm25}\\nMSE: {mse:.4f}\"\n",
    "    else:\n",
    "        return f\"Predicted PM2.5 for {city_name} on {future_date}: {pred_pm25}\\n(No actual data available for MSE calculation)\"\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=gradio_predict,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter City Name\", placeholder=\"E.g., New York\"),\n",
    "        gr.Textbox(label=\"Enter Future Date (YYYY-MM-DD)\", placeholder=\"E.g., 2025-03-01\"),\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Prediction Output\"),\n",
    "    title=\"ğŸŒ PM2.5 Prediction Dashboard\",\n",
    "    description=\"Predict PM2.5 levels for a given city using BiLSTM.\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anacondaa\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 95398.0547 - val_loss: 58008.0586\n",
      "Epoch 2/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 51642.0078 - val_loss: 53663.7344\n",
      "Epoch 3/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 50370.0820 - val_loss: 53842.3359\n",
      "Epoch 4/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 52237.2930 - val_loss: 53446.0547\n",
      "Epoch 5/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 53328.0703 - val_loss: 54110.2656\n",
      "Epoch 6/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 50414.6094 - val_loss: 53270.2734\n",
      "Epoch 7/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 53085.2812 - val_loss: 53556.9219\n",
      "Epoch 8/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 49965.8086 - val_loss: 53146.0078\n",
      "Epoch 9/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 51185.7305 - val_loss: 53361.9414\n",
      "Epoch 10/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 51553.8789 - val_loss: 53437.8398\n",
      "Epoch 11/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 52137.0117 - val_loss: 53640.5820\n",
      "Epoch 12/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 51911.2969 - val_loss: 53865.7227\n",
      "Epoch 13/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 49283.0859 - val_loss: 53202.7773\n",
      "Epoch 14/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 49735.1875 - val_loss: 53279.2539\n",
      "Epoch 15/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 50895.6797 - val_loss: 53242.0352\n",
      "Epoch 16/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 51773.9570 - val_loss: 53670.5156\n",
      "Epoch 17/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 50008.2305 - val_loss: 53016.8438\n",
      "Epoch 18/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 50996.3828 - val_loss: 53577.1289\n",
      "Epoch 19/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 52252.8750 - val_loss: 53491.6055\n",
      "Epoch 20/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 51403.1719 - val_loss: 53499.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "d:\\Anacondaa\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 560ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "d:\\Anacondaa\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "d:\\Anacondaa\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import joblib\n",
    "import pickle\n",
    "import gradio as gr\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    return df\n",
    "\n",
    "# Define file path\n",
    "file_path = r'C:\\Users\\ASUS\\Desktop\\Climate1\\delhi_aqi.csv'\n",
    "df = load_data(file_path)\n",
    "\n",
    "# Define target variable\n",
    "features = ['hour', 'day', 'month', 'year']\n",
    "target = 'pm2_5'\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, shuffle=False)\n",
    "\n",
    "# Normalize features for LSTM\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Train LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(1, 4)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=20, batch_size=32, validation_data=(X_test_lstm, y_test), verbose=1)\n",
    "\n",
    "# Save the models\n",
    "xgb_model.save_model(\"xgb_model.json\")\n",
    "lstm_model.save(\"lstm_model.h5\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Save ensemble model\n",
    "ensemble_model_data = {\n",
    "    \"xgb_model\": \"xgb_model.json\",\n",
    "    \"lstm_model\": \"lstm_model.h5\",\n",
    "    \"scaler\": \"scaler.pkl\",\n",
    "    \"weights\": (0.5, 0.5)\n",
    "}\n",
    "with open(\"ensemble_model3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ensemble_model_data, f)\n",
    "\n",
    "# Function to load the models\n",
    "def load_models():\n",
    "    with open(\"ensemble_model3.pkl\", \"rb\") as f:\n",
    "        ensemble_data = pickle.load(f)\n",
    "    \n",
    "    xgb_loaded = xgb.XGBRegressor()\n",
    "    xgb_loaded.load_model(ensemble_data[\"xgb_model\"])\n",
    "    lstm_loaded = load_model(ensemble_data[\"lstm_model\"])\n",
    "    scaler_loaded = joblib.load(ensemble_data[\"scaler\"])\n",
    "    weights = ensemble_data[\"weights\"]\n",
    "    \n",
    "    return xgb_loaded, lstm_loaded, scaler_loaded, weights\n",
    "\n",
    "# Function to predict PM2.5 for a future date\n",
    "def predict_future_pm25(future_date):\n",
    "    xgb_model, lstm_model, scaler, weights = load_models()\n",
    "    \n",
    "    # Convert future_date string to datetime object\n",
    "    future_date = pd.to_datetime(future_date)\n",
    "    \n",
    "    # Extract time-based features\n",
    "    future_features = [future_date.hour, future_date.day, future_date.month, future_date.year]\n",
    "    \n",
    "    # Preprocess input\n",
    "    future_scaled = scaler.transform([future_features])\n",
    "    future_lstm = future_scaled.reshape((1, 1, len(future_features)))\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_xgb = xgb_model.predict(future_scaled)[0]\n",
    "    pred_lstm = lstm_model.predict(future_lstm)[0][0]\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    pred_ensemble = (weights[0] * pred_xgb) + (weights[1] * pred_lstm)\n",
    "    \n",
    "    return f\"Predicted PM2.5 for {future_date}: {pred_ensemble:.2f}\"\n",
    "\n",
    "# Gradio interface\n",
    "def gradio_interface(future_date):\n",
    "    return predict_future_pm25(future_date)\n",
    "\n",
    "iface = gr.Interface(fn=gradio_interface, inputs=\"text\", outputs=\"text\", title=\"Future PM2.5 Prediction\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
